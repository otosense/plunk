{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main goal of pipeline maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want the user to specify a **collection of objects**\n",
    "* more concretely: \n",
    "  * connect a **datasource** to an existing dacc, in a normalized form (say, a dacc using WfStore, AnnotStore)\n",
    "  * datasource can be anything, but the (raw) values we want are numpy arrays (int16). \n",
    "  The keys can be deduced from the items, after a sequence of transformations (see mk_item2kv_for, and other functions in dol for example). \n",
    "  Another tool to use : dol/filt_iter on store\n",
    "  We can use the interface of dol/Files to get the raw material (example: FileBytesReader)\n",
    "\n",
    "* At the **GUI level**: user makes a collection by adding funcs\n",
    "  * 'select one' : from pre-existing dropdown list\n",
    "  * 'make one' and select after (will use crude: we select through the crude name)\n",
    "\n",
    "* At the **backend level**:\n",
    "  * two stores are provided: one store of funcs (populated with 2-3 funcs say), one store of factories.\n",
    "  * make a collection of functions through the GUI (in the form of Slabs or not)\n",
    "  * the user needs to populate the items. Behind the scenes we provide functions to create the list of objects that we can select. Those functions can be provided as keys, of a mall. If very large, the collection of keys is accessed via a smartlist. The function that creates the items could be as simple as dunder iter for the store.\n",
    "  * making one element: we provide a store of factories. Those have certain signatures, that will determine the front rendering of each. For example: one factory needs some params to be entered by the user, so front creates a page showing the arguments of the factory. User enters arguments, then click to save in (dill) store of functions.\n",
    "  * In summary: there are 2 stores, store of funcs, store of factories\n",
    "\n",
    "* **Remark**: crude:\n",
    "  does not appear at the level of the React components. \n",
    "\n",
    "* **example**:\n",
    "    make a pipeline to create a Slabs example (ie a store exists, and is transformed by adding more stuff to it)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments by Thor\n",
    "Looked through in a noisy env, so not the level of reading I would want, but from my current reading, yes, that’s it.\n",
    "Now you probably want to write a bit more code, like\n",
    "* sets of object and factory stores around our use cases (making stores (with dol wrappers), making train pipelines, making test pipelines, making things (slabiter instances) that will run on live streams)\n",
    "* what patterns emerge?\n",
    "* this is sort of a mini-framework to make GUIs (or for now backend to GUIs): What sort of tools would make this framework easier to use (for example, what kind of stores or store wrappers).\n",
    "* Misc questions.\n",
    "\n",
    "Here’s an example of a misc question: Should object and factory stores be separate?\n",
    "It make it nice and clean, yes, until…\n",
    "* You realize that the composite objects (e.g. Pipe instances or SlabIter objects) are… objects themselves that you’d like to reuse perhaps.\n",
    "* You realize that factories can be partialized (still factories though, but where do you save them?)\n",
    "So perhaps instead of two stores, we should have one (on the surface at least), but with a “kind” and ability to filter on it.\n",
    "For example… ("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Slabs example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* random generator: provided directly\n",
    "* clipper: clip above fixed value\n",
    "* threshold maker: a factory\n",
    "(* jump finder: a factory (define jump))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from know.base import Slabs\n",
    "\n",
    "\n",
    "DFLT_SIZE = 5\n",
    "DFLT_WIDTH = 10\n",
    "\n",
    "def random_gen(width= DFLT_WIDTH, size = DFLT_SIZE):\n",
    "    while True:\n",
    "        yield np.random.randint(width, size=size)\n",
    "\n",
    "def clipper(arr, a_min=0, a_max=10):\n",
    "    return np.clip(arr, a_min, a_max)\n",
    "\n",
    "def threshold_factory():\n",
    "    max_val = int(input())\n",
    "    return partial(clipper, a_min=0, a_max = max_val)\n",
    "\n",
    "\n",
    "data = random_gen().__next__\n",
    "clipper_4 = partial(clipper, a_min=0, a_max = 4)\n",
    "\n",
    "funcs_store = {'clipper': clipper_4}\n",
    "factories_store = {'threshold': threshold_factory}\n",
    "\n",
    "transform = funcs_store['clipper']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Slabs(\n",
    "    data = data, \n",
    "    transform = lambda data: data>3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([0, 8, 3, 4, 5]),\n",
       " 'transform': array([False,  True, False,  True,  True])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples centered around our use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sets of object and factory stores around our use cases (making stores (with dol wrappers), making train pipelines, making test pipelines, making things (slabiter instances) that will run on live streams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "from dol.appendable import add_append_functionality_to_store_cls\n",
    "from dol import Store\n",
    "from dol import FilesOfZip, wrap_kvs, filt_iter\n",
    "\n",
    "from py2store import FilesOfZip\n",
    "from hear import WavLocalFileStore\n",
    "from dol import FuncReader\n",
    "\n",
    "\n",
    "\n",
    "def my_obj_of_data(b):\n",
    "    return sf.read(BytesIO(b), dtype=\"float32\")[0]\n",
    "\n",
    "@wrap_kvs(obj_of_data=my_obj_of_data)\n",
    "@filt_iter(filt=lambda x: not x.startswith(\"__MACOSX\") and x.endswith(\".wav\"))\n",
    "class WfZipStore(FilesOfZip):\n",
    "    \"\"\"Waveform access. Keys are .wav filenames and values are numpy arrays of int16 waveform.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def key_to_ext(k):\n",
    "    _, ext = os.path.splitext(k)\n",
    "    if ext.startswith(\".\"):\n",
    "        ext = ext[1:]\n",
    "    return ext\n",
    "\n",
    "\n",
    "def processor_from_ext(ext):\n",
    "    if ext.startswith(\".\"):\n",
    "        ext = ext[1:]\n",
    "    if ext in {\"zip\"}:\n",
    "        pass\n",
    "    elif ext in {\"wav\"}:\n",
    "        pass\n",
    "\n",
    "def is_zip_file(filepath):\n",
    "    return zipfile.is_zipfile(filepath)\n",
    "\n",
    "def is_dir(filepath):\n",
    "    return os.path.isdir(filepath)\n",
    "\n",
    "def key_maker(name, prefix):\n",
    "    return f'{prefix}_{name}'\n",
    "\n",
    "def wf_store_factory(filepath):\n",
    "    key = key_maker(name = filepath, prefix='wf_store')\n",
    "    tag = 'wf_store'\n",
    "\n",
    "    if is_dir(filepath):\n",
    "        data = WavLocalFileStore(filepath)\n",
    "\n",
    "         \n",
    "    elif is_zip_file(filepath):\n",
    "        data = WfZipStore(filepath)\n",
    "\n",
    "    return mk_store_item(key, tag, data)\n",
    "\n",
    "def annot_store_factory(filepath):\n",
    "    key = key_maker(name = filepath, prefix='annot_store')\n",
    "    tag = 'annot_store'\n",
    "\n",
    "    data = pd.read_csv(filepath)\n",
    "\n",
    "    return mk_store_item(key, tag, data)\n",
    "\n",
    "def mk_store_item(key, tag, data):\n",
    "    return dict(key = key, tag = tag, data=data)\n",
    "\n",
    "def append_to_store(store, item):\n",
    "    store.append(item)\n",
    "\n",
    "def dacc_factory():\n",
    "    pass\n",
    "\n",
    "factory_store = {'wf_store': wf_store_factory, 'dacc':None}\n",
    "\n",
    "factory_store = FuncReader([wf_store_factory, dacc_factory])\n",
    "\n",
    "store_cls = Store\n",
    "item2kv = lambda item: (item['key'], item)\n",
    "\n",
    "appendable_store_cls = add_append_functionality_to_store_cls(store_cls, item2kv=item2kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "rootdir = '/Users/sylvain/Dropbox/Otosense/VacuumEdgeImpulse/'\n",
    "annot_filepath = '/Users/sylvain/Dropbox/sipyb/Testing/data/annots_vacuum.csv'\n",
    "is_dir(rootdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user story\n",
    "global_store = appendable_store_cls()\n",
    "\n",
    "wf_store_item = wf_store_factory(filepath=rootdir)\n",
    "global_store.append(wf_store_item)\n",
    "annot_store_item = annot_store_factory(filepath = annot_filepath)\n",
    "global_store.append(annot_store_item)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wf_store_/Users/sylvain/Dropbox/Otosense/VacuumEdgeImpulse/',\n",
       " 'annot_store_/Users/sylvain/Dropbox/sipyb/Testing/data/annots_vacuum.csv']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(global_store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnnotStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "from hear import WavLocalFileStore\n",
    "from dol import wrap_kvs\n",
    "import soundfile as sf\n",
    "from io import BytesIO\n",
    "from odat.utils.chunkers import fixed_step_chunker\n",
    "from slang.featurizers import tile_fft\n",
    "import pandas as pd\n",
    "from odat.mdat.vacuum import annot_columns, DFLT_ANNOTS_COLS\n",
    "\n",
    "DFLT_CHUNKER = partial(fixed_step_chunker, chk_size=2048)\n",
    "DFLT_FEATURIZER = tile_fft\n",
    "\n",
    "\n",
    "class Dacc:\n",
    "    def __init__(self, wf_store):\n",
    "        self.wfs = wf_store\n",
    "\n",
    "    def mk_annots(self):\n",
    "        srefs = self.wfs.keys()\n",
    "        annots = annot_columns(srefs)\n",
    "        return annots\n",
    "\n",
    "    @property\n",
    "    def annots_df(self):\n",
    "        annots = self.mk_annots()\n",
    "        columns = DFLT_ANNOTS_COLS\n",
    "        df = pd.DataFrame(annots, columns=columns)\n",
    "        return df\n",
    "\n",
    "    def wf_tag_train_gen(self):\n",
    "        for key in self.wfs:\n",
    "            signal = self.wfs[key]\n",
    "            train = key.split(\"/\")[0]\n",
    "            tag = key.split(\"/\")[1].split(\".\")[0]\n",
    "            normal_wf = normalize(np.float32(signal).reshape(1, -1))[0]\n",
    "\n",
    "            yield normal_wf, tag, train\n",
    "\n",
    "    def chk_tag_train_gen(self, chunker=DFLT_CHUNKER):\n",
    "        for wf, tag, train in self.wf_tag_train_gen():\n",
    "            for chk in chunker(wf):\n",
    "                yield chk, tag, train\n",
    "\n",
    "    def fvs_tag_train_gen(self, featurizer=DFLT_FEATURIZER):\n",
    "        for chk, tag, train in self.chk_tag_train_gen():\n",
    "            yield featurizer(chk), tag, train\n",
    "\n",
    "    def mk_Xy(self):  # TODO use a groupby here\n",
    "        X_train, y_train, X_test, y_test = [], [], [], []\n",
    "        for fv, tag, train in self.fvs_tag_train_gen():\n",
    "            if train == \"train\":\n",
    "                X_train.append(fv)\n",
    "                y_train.append(tag)\n",
    "            elif train == \"test\":\n",
    "                X_test.append(fv)\n",
    "                y_test.append(tag)\n",
    "            else:\n",
    "                continue\n",
    "        return np.array(X_train), y_train, np.array(X_test), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mannot_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
      "\u001b[0;31mSource:\u001b[0m   \n",
      "\u001b[0;32mdef\u001b[0m \u001b[0mannot_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_annot_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrefs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m      ~/Desktop/dev/otosense/odat/odat/mdat/vacuum.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "annot_columns??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factory\n",
    "from functools import singledispatch\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DirZips:\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DirWavs:\n",
    "    pass\n",
    "\n",
    "\n",
    "@singledispatch\n",
    "def process(obj=None):\n",
    "    raise NotImplementedError(\"Can't create a SourceStore from that directory\")\n",
    "\n",
    "\n",
    "@process.register(DirZips)\n",
    "def sub_process(obj):\n",
    "    return \"DirZips processed successfully!\"\n",
    "\n",
    "\n",
    "@process.register(DirWavs)\n",
    "def sub_process(obj):\n",
    "    return \"DirWavs processed successfully!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('otopy310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b1583496da055986182577c8a7f701c9cd62d32bb22c260ed492920d961d72e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
